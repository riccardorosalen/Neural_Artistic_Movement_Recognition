{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bc6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tq\n",
    "import time\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9cd03",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "\n",
    "There are 6 folders containing pictures divided into the 6 movements\n",
    "\n",
    "Labels will be one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decfdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"./Data/faces_6class\"\n",
    "\n",
    "class_folders = sorted(os.listdir(main_folder_path))\n",
    "num_classes = len(class_folders)\n",
    "\n",
    "labels = {}  # Dictionary to map class names to one-hot labels\n",
    "for i, class_folder in enumerate(class_folders):\n",
    "    labels[class_folder] = i\n",
    "\n",
    "images = []\n",
    "one_hot_labels = []\n",
    "\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(main_folder_path, class_folder)\n",
    "    class_label = labels[class_folder]\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        #print(image_file)\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Process the image as needed (e.g., resize, normalize)\n",
    "        img = img.resize((400, 400))\n",
    "        \n",
    "        #print(img.size)\n",
    "                \n",
    "        # Append the image to the list of images\n",
    "        images.append(np.array(img))\n",
    "        \n",
    "        # Create a one-hot label vector\n",
    "        one_hot = np.zeros(num_classes)\n",
    "        one_hot[class_label] = 1\n",
    "        one_hot_labels.append(one_hot)\n",
    "\n",
    "# Convert images and labels to numpy arrays\n",
    "images = np.array(images)\n",
    "one_hot_labels = np.array(one_hot_labels)\n",
    "\n",
    "# Now, you have your images in the 'images' array and corresponding one-hot label vectors in 'one_hot_labels'.\n",
    "# You can use these data for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e669",
   "metadata": {},
   "source": [
    "# Shuffle the dataset\n",
    "Since the images are divided by class, we shuffle the dataset to have a sparser dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eda000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples\n",
    "num_samples = len(images)\n",
    "\n",
    "# Create a permutation index\n",
    "permutation = np.random.permutation(num_samples)\n",
    "\n",
    "# Shuffle both 'images' and 'labels' using the same permutation\n",
    "shuffled_images = images[permutation]\n",
    "shuffled_labels = one_hot_labels[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56b05",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da49665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class DatasetRGB(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        element = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        element = torch.tensor(element) \n",
    "                \n",
    "        return element, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9250f7",
   "metadata": {},
   "source": [
    "# Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb800ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, y_validation, RGB = False, is_ensamble = False, normalized_cm = False):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "    i=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, label in validation_loader:\n",
    "            \n",
    "            \n",
    "            \n",
    "            #predict label\n",
    "            if RGB==True:\n",
    "                output = model(inputs.permute(0, 3, 1, 2))\n",
    "            else:\n",
    "                output = model(inputs)\n",
    "           \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            \n",
    "            correct += (max_index == torch.argmax(label).item())\n",
    "            i+=1\n",
    "\n",
    "    \n",
    "    accuracy = 100 * correct / len(y_validation)\n",
    "    average_loss = total_loss / len(y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdded17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, validation_dataset, batch_size, num_epochs, learning_rate, verbose = False,\n",
    "          reg=1e-5, RGB = False, is_ensamble = False, normalized_cm=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "          \n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            if RGB==True:\n",
    "                inputs = inputs.permute(0, 3, 1, 2)\n",
    "            outputs = model(inputs)\n",
    "                        \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                if(torch.argmax(labels[index]).item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss,_ = test(model, validation_dataset, y_validation, RGB = RGB, is_ensamble = is_ensamble, normalized_cm=False)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e39e93",
   "metadata": {},
   "source": [
    "# Split into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 'shuffled_images' and 'shuffled_labels'\n",
    "\n",
    "# Define the proportions for the splits (e.g., 60% train, 20% validation, 20% test)\n",
    "train_percent = 0.6\n",
    "validation_percent = 0.2\n",
    "\n",
    "num_samples = len(shuffled_images)\n",
    "\n",
    "train_size = int(train_percent * num_samples)\n",
    "validation_size = int(validation_percent * num_samples)\n",
    "\n",
    "x_train, x_validation, x_test = (\n",
    "    shuffled_images[:train_size],\n",
    "    shuffled_images[train_size:train_size + validation_size],\n",
    "    shuffled_images[train_size + validation_size:]\n",
    ")\n",
    "\n",
    "y_train, y_validation, y_test = (\n",
    "    shuffled_labels[:train_size],\n",
    "    shuffled_labels[train_size:train_size + validation_size],\n",
    "    shuffled_labels[train_size + validation_size:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3535b4",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8db6a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Raw_Images_NN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Raw_Images_NN, self).__init__()        \n",
    "        # First convolutional layer with batch normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization for the first convolutional layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=8)\n",
    "        # First convolutional layer with batch normalization\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization for the first convolutional layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=4)\n",
    "        \n",
    "        \n",
    "        # Fully connected layers with batch normalization and dropout\n",
    "        self.fc1 = nn.Linear(7744, 128)  # Adjust the input size accordingly\n",
    "        self.bn3 = nn.BatchNorm1d(128)  # Batch normalization for the first linear layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.4)  # Dropout after the first linear layer\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)  # Add another linear layer\n",
    "        self.bn4 = nn.BatchNorm1d(64)  # Batch normalization for the second linear layer\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout after the second linear layer\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 32)  # Add one more linear layer\n",
    "        self.bn5 = nn.BatchNorm1d(32)  # Batch normalization for the third linear layer\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)  # Dropout after the third linear layer\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, num_classes)  # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)  # Apply batch normalization\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn4(x)  # Apply batch normalization\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.bn5(x)  # Apply batch normalization\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout3(x)  # Apply dropout\n",
    "        \n",
    "        x = self.fc5(x)  # Output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9be8d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=DatasetRGB(x_train,y_train)\n",
    "validation_set=DatasetRGB(x_validation,y_validation)\n",
    "test_set=DatasetRGB(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1aa807d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0523b0613ed426e86e289de5142586c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2353e502a996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRaw_Images_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f8d911c98acb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, validation_dataset, batch_size, num_epochs, learning_rate, verbose, reg, RGB, is_ensamble, normalized_cm)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ab0242869cfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=Raw_Images_NN(6)\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_set,validation_set, batch_size=16, num_epochs=50, learning_rate=0.001, reg=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426514b2",
   "metadata": {},
   "source": [
    "# Use of Features\n",
    "\n",
    "## HSV Color Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1c985de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def HSVprocess(tmpimage,variance):\n",
    "    #print(\"A\")\n",
    "    image = rgb2hsv(tmpimage)\n",
    "    vert, oriz, plane = image.shape\n",
    "    MAPPING = None  # You can provide the LBP mapping here if needed\n",
    "\n",
    "    # Initialize ratio images\n",
    "    hs = np.zeros((vert, oriz))\n",
    "    sv = np.zeros((vert, oriz))\n",
    "    hv = np.zeros((vert, oriz))\n",
    "\n",
    "    # Get ratio values of each pixel\n",
    "    for i in range(vert):\n",
    "        for j in range(oriz):\n",
    "            if variance == 1:\n",
    "                sumH, sumS, sumV = 0, 0, 0\n",
    "\n",
    "                # Bit in the middle\n",
    "                sumH += image[i, j, 0]\n",
    "                sumS += image[i, j, 1]\n",
    "                sumV += image[i, j, 2]\n",
    "\n",
    "                # Check other bits around to avoid errors and sum them if possible\n",
    "                if i > 0:\n",
    "                    sumH += image[i - 1, j, 0]\n",
    "                    sumS += image[i - 1, j, 1]\n",
    "                    sumV += image[i - 1, j, 2]\n",
    "\n",
    "                    if j > 0:\n",
    "                        sumH += image[i - 1, j - 1, 0]\n",
    "                        sumS += image[i - 1, j - 1, 1]\n",
    "                        sumV += image[i - 1, j - 1, 2]\n",
    "\n",
    "                    if j < oriz - 1:\n",
    "                        sumH += image[i - 1, j + 1, 0]\n",
    "                        sumS += image[i - 1, j + 1, 1]\n",
    "                        sumV += image[i - 1, j + 1, 2]\n",
    "\n",
    "                if i < vert - 1:\n",
    "                    sumH += image[i + 1, j, 0]\n",
    "                    sumS += image[i + 1, j, 1]\n",
    "                    sumV += image[i + 1, j, 2]\n",
    "\n",
    "                    if j > 0:\n",
    "                        sumH += image[i + 1, j - 1, 0]\n",
    "                        sumS += image[i + 1, j - 1, 1]\n",
    "                        sumV += image[i + 1, j - 1, 2]\n",
    "\n",
    "                    if j < oriz - 1:\n",
    "                        sumH += image[i + 1, j + 1, 0]\n",
    "                        sumS += image[i + 1, j + 1, 1]\n",
    "                        sumV += image[i + 1, j + 1, 2]\n",
    "\n",
    "                if j < oriz - 1:\n",
    "                    sumH += image[i, j + 1, 0]\n",
    "                    sumS += image[i, j + 1, 1]\n",
    "                    sumV += image[i, j + 1, 2]\n",
    "\n",
    "                if j > 0:\n",
    "                    sumH += image[i, j - 1, 0]\n",
    "                    sumS += image[i, j - 1, 1]\n",
    "                    sumV += image[i, j - 1, 2]\n",
    "\n",
    "                if i > 1:\n",
    "                    sumH += image[i - 2, j, 0]\n",
    "                    sumS += image[i - 2, j, 1]\n",
    "                    sumV += image[i - 2, j, 2]\n",
    "\n",
    "                    if j > 0:\n",
    "                        sumH += image[i - 2, j - 1, 0]\n",
    "                        sumS += image[i - 2, j - 1, 1]\n",
    "                        sumV += image[i - 2, j - 1, 2]\n",
    "\n",
    "                    if j < oriz - 1:\n",
    "                        sumH += image[i - 2, j + 1, 0]\n",
    "                        sumS += image[i - 2, j + 1, 1]\n",
    "                        sumV += image[i - 2, j + 1, 2]\n",
    "\n",
    "                    if j > 1:\n",
    "                        sumH += image[i - 2, j - 2, 0]\n",
    "                        sumS += image[i - 2, j - 2, 1]\n",
    "                        sumV += image[i - 2, j - 2, 2]\n",
    "\n",
    "                    if j < oriz - 2:\n",
    "                        sumH += image[i - 2, j + 2, 0]\n",
    "                        sumS += image[i - 2, j + 2, 1]\n",
    "                        sumV += image[i - 2, j + 2, 2]\n",
    "\n",
    "                if i < vert - 2:\n",
    "                    sumH += image[i + 2, j, 0]\n",
    "                    sumS += image[i + 2, j, 1]\n",
    "                    sumV += image[i + 2, j, 2]\n",
    "\n",
    "                    if j > 0:\n",
    "                        sumH += image[i + 2, j - 1, 0]\n",
    "                        sumS += image[i + 2, j - 1, 1]\n",
    "                        sumV += image[i + 2, j - 1, 2]\n",
    "\n",
    "                    if j < oriz - 1:\n",
    "                        sumH += image[i + 2, j + 1, 0]\n",
    "                        sumS += image[i + 2, j + 1, 1]\n",
    "                        sumV += image[i + 2, j + 1, 2]\n",
    "\n",
    "                    if j > 1:\n",
    "                        sumH += image[i + 2, j - 2, 0]\n",
    "                        sumS += image[i + 2, j - 2, 1]\n",
    "                        sumV += image[i + 2, j - 2, 2]\n",
    "\n",
    "                    if j < oriz - 2:\n",
    "                        sumH += image[i + 2, j + 2, 0]\n",
    "                        sumS += image[i + 2, j + 2, 1]\n",
    "                        sumV += image[i + 2, j + 2, 2]\n",
    "\n",
    "                if j > 1:\n",
    "                    sumH += image[i, j - 2, 0]\n",
    "                    sumS += image[i, j - 2, 1]\n",
    "                    sumV += image[i, j - 2, 2]\n",
    "\n",
    "                    if i > 0:\n",
    "                        sumH += image[i - 1, j - 2, 0]\n",
    "                        sumS += image[i - 1, j - 2, 1]\n",
    "                        sumV += image[i - 1, j - 2, 2]\n",
    "\n",
    "                    if i < vert - 1:\n",
    "                        sumH += image[i + 1, j - 2, 0]\n",
    "                        sumS += image[i + 1, j - 2, 1]\n",
    "                        sumV += image[i + 1, j - 2, 2]\n",
    "\n",
    "                if j < oriz - 2:\n",
    "                    sumH += image[i, j + 2, 0]\n",
    "                    sumS += image[i, j + 2, 1]\n",
    "                    sumV += image[i, j + 2, 2]\n",
    "\n",
    "                    if i > 0:\n",
    "                        sumH += image[i - 1, j + 2, 0]\n",
    "                        sumS += image[i - 1, j + 2, 1]\n",
    "                        sumV += image[i - 1, j + 2, 2]\n",
    "\n",
    "                    if i < vert - 1:\n",
    "                        sumH += image[i + 1, j + 2, 0]\n",
    "                        sumS += image[i + 1, j + 2, 1]\n",
    "                        sumV += image[i + 1, j + 2, 2]\n",
    "\n",
    "                # New Pixel Value, calculated with the ratio of the sum\n",
    "                hs[i, j] = np.log((sumH + 1e-12) / (sumS + 1e-12))\n",
    "                sv[i, j] = np.log((sumS + 1e-12) / (sumV + 1e-12))\n",
    "                hv[i, j] = np.log((sumH + 1e-12) / (sumV + 1e-12))\n",
    "\n",
    "            elif variance == 2:\n",
    "                # Second variance of the feature, there's no window, the ratio goes within the pixel\n",
    "                hs[i, j] = np.log((image[i, j, 0] + 1e-12) / (image[i, j, 1] + 1e-12))\n",
    "                sv[i, j] = np.log((image[i, j, 1] + 1e-12) / (image[i, j, 2] + 1e-12))\n",
    "                hv[i, j] = np.log((image[i, j, 0] + 1e-12) / (image[i, j, 2] + 1e-12))\n",
    "\n",
    "    # Get LBP feature vector of each ratio\n",
    "    lbphs = local_binary_pattern(hs, 8, 1, method='uniform')\n",
    "    lbphv = local_binary_pattern(hv, 8, 1, method='uniform')\n",
    "    lbpsv = local_binary_pattern(sv, 8, 1, method='uniform')\n",
    "\n",
    "    # Compute LBP histograms for each LBP map\n",
    "    hist_hs, _ = np.histogram(lbphs, bins=np.arange(0, 256), density=True)\n",
    "    hist_hv, _ = np.histogram(lbphv, bins=np.arange(0, 256), density=True)\n",
    "    hist_sv, _ = np.histogram(lbpsv, bins=np.arange(0, 256), density=True)\n",
    "    \n",
    "    # Concatenate the three histograms into one feature vector\n",
    "    tmp = np.concatenate((hist_hs, hist_hv, hist_sv))\n",
    "\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbff6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n"
     ]
    }
   ],
   "source": [
    "x_train_ratio=[]\n",
    "count=0\n",
    "for x in x_train:\n",
    "    count=count+1\n",
    "    print (count)\n",
    "    x_train_ratio.append(HSVprocess(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e1af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_ratio=[]\n",
    "for x in x_validation:\n",
    "    x_validation_ratio.append(HSVprocess(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1c5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "x_test_ratio=[]\n",
    "count=0\n",
    "for x in x_test:\n",
    "    count=count+1\n",
    "    print (count)\n",
    "    x_test_ratio.append(HSVprocess(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1777021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ratio_NN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Ratio_NN, self).__init__()        \n",
    "        # First convolutional layer with batch normalization\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        # Second convolutional layer with batch normalization\n",
    "        self.conv2d = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3d = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=8)\n",
    "        \n",
    "        self.conv4d = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Fully connected layers with batch normalization and dropout\n",
    "        self.fc1 = nn.Linear(6080, 128)  # Adjust the input size accordingly\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x=self.maxpool1()\n",
    "        x = self.conv1d((x.float()).unsqueeze(1))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SimpleNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "\n",
    "        # Define the layers of the network\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=8)\n",
    "        self.bnc1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8)\n",
    "        self.bnc2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=8)\n",
    "        self.bnc3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=8)\n",
    "        self.bnc4 = nn.BatchNorm1d(128)\n",
    "        self.conv5 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8)\n",
    "        self.bnc5 = nn.BatchNorm1d(128)\n",
    "        self.linear1 = torch.nn.Linear(1376, 512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.dropout1 = torch.nn.Dropout(0.4)\n",
    "        self.linear2 = torch.nn.Linear(512, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.linear3 = torch.nn.Linear(256, 6)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.maxpool=nn.MaxPool1d(kernel_size=16)\n",
    "        self.maxpool1=nn.MaxPool1d(kernel_size=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.unsqueeze(1)\n",
    "        # Pass the input through the convolutional layer\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bnc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bnc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bnc4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bnc5(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Flatten the output of the convolutional layer\n",
    "        x = self.flatten(x)\n",
    "        x=self.maxpool(x)\n",
    "        # Pass the flattened output through the linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f5d50",
   "metadata": {},
   "source": [
    "# Saving into files the dataset to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "952d5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into files the generated datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving into files the generated datasets\")\n",
    "np.save(\"x_train\",x_train_ratio)\n",
    "np.save(\"x_validation\",x_validation_ratio)\n",
    "np.save(\"x_test\",x_test_ratio)\n",
    "np.save(\"y_train\",y_train)\n",
    "np.save(\"y_validation\",y_validation)\n",
    "np.save(\"y_test\",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d630f",
   "metadata": {},
   "source": [
    "# Set splitting for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa94cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train_set=DatasetRGB(x_train_ratio,y_train)\n",
    "ratio_validation_set=DatasetRGB(x_validation_ratio,y_validation)\n",
    "#ratio_test_set=DatasetRGB(x_test_ratio,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94a09cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26b9d8a10be4c24bd29e38596e0e66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50],Train Loss: 1.8965. Train Accuracy: 15.535714285714286 Val Loss: 1.985518991624987 Val Accuracy: 15.675675675675675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fdff30ca7541ce8656905fdf91e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50],Train Loss: 1.7921. Train Accuracy: 24.107142857142858 Val Loss: 1.884038974787738 Val Accuracy: 11.35135135135135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd326144d56451a9e77ca74b1362095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50],Train Loss: 1.7752. Train Accuracy: 25.357142857142858 Val Loss: 1.639037416432355 Val Accuracy: 34.054054054054056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1618662e9994445921082e673b05ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50],Train Loss: 1.7262. Train Accuracy: 27.321428571428573 Val Loss: 1.586983442950893 Val Accuracy: 44.32432432432432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dd885a51d84dd6af9c4f28c2c84bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50],Train Loss: 1.6903. Train Accuracy: 29.821428571428573 Val Loss: 1.5591337091213948 Val Accuracy: 43.78378378378378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c00abacbccf4bf0a834a872bfc73325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50],Train Loss: 1.6804. Train Accuracy: 28.035714285714285 Val Loss: 1.5351256096685255 Val Accuracy: 44.86486486486486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a17029a1ef24b17a5eaf6d87ee06414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50],Train Loss: 1.6294. Train Accuracy: 34.107142857142854 Val Loss: 1.5294800220309077 Val Accuracy: 46.486486486486484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798ead889c1f4b9eaacb730e5db27ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50],Train Loss: 1.6486. Train Accuracy: 30.892857142857142 Val Loss: 1.507539447094943 Val Accuracy: 47.027027027027025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed1669b4f0c4cf6a4c59c30eccb6098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50],Train Loss: 1.5963. Train Accuracy: 33.92857142857143 Val Loss: 1.492549862893852 Val Accuracy: 47.027027027027025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fdb5db1771438199a475ddd1a9d195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50],Train Loss: 1.5913. Train Accuracy: 35.535714285714285 Val Loss: 1.4718278614250389 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c45688af63e4a31b2ea0cd4d054ca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50],Train Loss: 1.6029. Train Accuracy: 36.964285714285715 Val Loss: 1.4673879254508664 Val Accuracy: 45.945945945945944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a1c58c771f4fdfacd3c5050e32bc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50],Train Loss: 1.5601. Train Accuracy: 39.82142857142857 Val Loss: 1.4481070452445262 Val Accuracy: 47.027027027027025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c15baec0d26416bbdcda4bbc656ddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50],Train Loss: 1.5381. Train Accuracy: 40.535714285714285 Val Loss: 1.4478812122667157 Val Accuracy: 47.567567567567565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f79d6b47f8423eb3388f79a241d66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50],Train Loss: 1.5492. Train Accuracy: 38.92857142857143 Val Loss: 1.442676293447211 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd56cfaa6fb402d805fde7b69bf93c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50],Train Loss: 1.5222. Train Accuracy: 40.17857142857143 Val Loss: 1.4300642526633032 Val Accuracy: 49.189189189189186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50efa731a72d48a59ae8752502fc25d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50],Train Loss: 1.5382. Train Accuracy: 40.535714285714285 Val Loss: 1.422366814113952 Val Accuracy: 48.648648648648646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e5d7bc8638421d9fcb6d45c233de3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50],Train Loss: 1.4830. Train Accuracy: 41.607142857142854 Val Loss: 1.4156438039766777 Val Accuracy: 49.729729729729726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd4a2b065e45b6bf303b105a7e683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50],Train Loss: 1.4617. Train Accuracy: 43.035714285714285 Val Loss: 1.4150705472843068 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce6cd40df05460bacbbc13ce96e8a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50],Train Loss: 1.5075. Train Accuracy: 37.67857142857143 Val Loss: 1.393555638677365 Val Accuracy: 47.567567567567565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d6b8a2195e4870bf0726c7ee416ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50],Train Loss: 1.4815. Train Accuracy: 41.785714285714285 Val Loss: 1.3885823781425888 Val Accuracy: 51.891891891891895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c3acaaaeaf44f2a937465db5f63891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50],Train Loss: 1.4740. Train Accuracy: 40.17857142857143 Val Loss: 1.3784486818957973 Val Accuracy: 51.351351351351354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdb93a0328845a9a52f71fae1514cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50],Train Loss: 1.4651. Train Accuracy: 43.57142857142857 Val Loss: 1.3737676017187737 Val Accuracy: 50.810810810810814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dd404e1da842858ca9983d2b25dbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50],Train Loss: 1.4614. Train Accuracy: 43.75 Val Loss: 1.3743021573569323 Val Accuracy: 51.351351351351354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979dc5e6d5d44119a4db951ebb7ed0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50],Train Loss: 1.4278. Train Accuracy: 45.17857142857143 Val Loss: 1.366046675878602 Val Accuracy: 48.648648648648646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c180d6dadbcc461686bccdf719a2694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50],Train Loss: 1.4136. Train Accuracy: 45.17857142857143 Val Loss: 1.3699430360181912 Val Accuracy: 47.027027027027025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd48c9a917344569e171ae71ec3858f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50],Train Loss: 1.4351. Train Accuracy: 42.5 Val Loss: 1.3608953803777695 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff267b325a4411fb64f31fc41612825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50],Train Loss: 1.4103. Train Accuracy: 43.92857142857143 Val Loss: 1.3616674029746572 Val Accuracy: 49.189189189189186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88498b248a7d427cafdf5306ad2a0ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50],Train Loss: 1.3875. Train Accuracy: 43.214285714285715 Val Loss: 1.3681290171839096 Val Accuracy: 48.648648648648646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d842460559c4d44bf395fa330972890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50],Train Loss: 1.3719. Train Accuracy: 48.214285714285715 Val Loss: 1.3425032917712185 Val Accuracy: 49.189189189189186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d952982724405381001bcfa1eb3688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50],Train Loss: 1.3952. Train Accuracy: 45.892857142857146 Val Loss: 1.3513670607595831 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bd7e4544734847981d82489f74630a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50],Train Loss: 1.3682. Train Accuracy: 48.214285714285715 Val Loss: 1.3458609877204573 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7657b7ccb5024307928c543ff1569403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50],Train Loss: 1.3917. Train Accuracy: 45.0 Val Loss: 1.3425962741713267 Val Accuracy: 46.486486486486484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce70b0d984394f7ead966de5f210280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50],Train Loss: 1.3741. Train Accuracy: 46.964285714285715 Val Loss: 1.343041573847468 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add1893dbb434de8b7412f59b112a98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50],Train Loss: 1.3703. Train Accuracy: 48.214285714285715 Val Loss: 1.3254066943518212 Val Accuracy: 49.189189189189186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d79622751074e8982d6c713bd94de53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50],Train Loss: 1.3462. Train Accuracy: 48.392857142857146 Val Loss: 1.3130699734027322 Val Accuracy: 48.648648648648646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370a54aff4724308bc85c036af2d9608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50],Train Loss: 1.3761. Train Accuracy: 47.857142857142854 Val Loss: 1.3316514120915453 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a39fa4cac54413fb40e34a83b8d112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50],Train Loss: 1.3019. Train Accuracy: 52.32142857142857 Val Loss: 1.3236690901823946 Val Accuracy: 49.729729729729726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0725a2e7c0412691e7ba2d540628a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50],Train Loss: 1.3412. Train Accuracy: 46.25 Val Loss: 1.3213327372194947 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4efd28c039441b99b7123b9e6cc6856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50],Train Loss: 1.3555. Train Accuracy: 47.142857142857146 Val Loss: 1.3112951895373093 Val Accuracy: 51.351351351351354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dfae1b4345477fbecd98a7c2b49397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50],Train Loss: 1.3143. Train Accuracy: 51.607142857142854 Val Loss: 1.2987637490436839 Val Accuracy: 48.648648648648646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5636a6f799348e69e2b491771ca2bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50],Train Loss: 1.3071. Train Accuracy: 49.285714285714285 Val Loss: 1.316484019140134 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f584cef1a8144fb886e4a9faf6820c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50],Train Loss: 1.3044. Train Accuracy: 50.535714285714285 Val Loss: 1.3064882480816262 Val Accuracy: 49.189189189189186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834f769c7df94bd68d37a8e11da7f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50],Train Loss: 1.3020. Train Accuracy: 47.32142857142857 Val Loss: 1.3077611479002076 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bec832c620498f93b81a69a538041d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50],Train Loss: 1.3151. Train Accuracy: 49.642857142857146 Val Loss: 1.3101040107172888 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129e9650892d4f1796bafd967ad04a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50],Train Loss: 1.3125. Train Accuracy: 50.535714285714285 Val Loss: 1.3017633033161227 Val Accuracy: 49.729729729729726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b24559467114ad9bc2c040b3ebeea41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50],Train Loss: 1.3124. Train Accuracy: 50.892857142857146 Val Loss: 1.3006396938518092 Val Accuracy: 48.108108108108105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f80d2a083a843708aa9599d516622d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50],Train Loss: 1.2830. Train Accuracy: 49.642857142857146 Val Loss: 1.298457635929053 Val Accuracy: 50.810810810810814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859fbe79407c4e8e8494a00d48bace71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50],Train Loss: 1.2714. Train Accuracy: 50.17857142857143 Val Loss: 1.3022984232954882 Val Accuracy: 49.729729729729726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e792ee8a0145e29cf598261fe5aa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50],Train Loss: 1.2728. Train Accuracy: 50.535714285714285 Val Loss: 1.290138479402742 Val Accuracy: 50.270270270270274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26194afeb8364f4389d9c0866fd663cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50],Train Loss: 1.2645. Train Accuracy: 53.035714285714285 Val Loss: 1.3045847538455917 Val Accuracy: 51.891891891891895\n"
     ]
    }
   ],
   "source": [
    "model=SimpleNetwork()\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, ratio_train_set,ratio_validation_set, batch_size=16, num_epochs=50, learning_rate=1e-5, reg=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09312ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(val_acc_list)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"My Plot\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6cad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
